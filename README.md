# EXAONE 3.5 Markdown Translator

EXAONE 3.5-7.8B ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ì—¬ í•œêµ­ì–´ ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œë¥¼ ì˜ì–´ë¡œ ë²ˆì—­í•˜ëŠ” í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

## Features

- **ë§ˆí¬ë‹¤ìš´ êµ¬ì¡° ë³´ì¡´**: ì½”ë“œ ë¸”ë¡, ë§í¬, í…Œì´ë¸” ë“± ë§ˆí¬ë‹¤ìš´ ìš”ì†Œë¥¼ ë³´ì¡´í•˜ë©° ë²ˆì—­
- **QLoRA ì§€ì›**: 4-bit ì–‘ìí™”ë¡œ RTX 3090/4080ì—ì„œë„ í•™ìŠµ ê°€ëŠ¥
- **ë‹¤ì–‘í•œ í‰ê°€ ë©”íŠ¸ë¦­**: BLEU, chrF, COMET + ë§ˆí¬ë‹¤ìš´ ë³´ì¡´ìœ¨ í‰ê°€

## Requirements

- Python 3.11+
- CUDA 11.8+
- GPU: 12GB+ VRAM (QLoRA), 24GB+ VRAM (LoRA)

## Installation

```bash
# ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv venv
source venv/bin/activate  # Linux/Mac
# or
.\venv\Scripts\activate  # Windows

# ì˜ì¡´ì„± ì„¤ì¹˜
python -m pip install -r requirements.txt
```

## Project Structure

```
â”œâ”€â”€ configs/                 # ì„¤ì • íŒŒì¼
â”‚   â”œâ”€â”€ qlora_config.yaml   # QLoRA ì„¤ì •
â”‚   â”œâ”€â”€ lora_config.yaml    # LoRA ì„¤ì •
â”‚   â”œâ”€â”€ training_config.yaml # í•™ìŠµ ì„¤ì •
â”‚   â”œâ”€â”€ inference_config.yaml # ì¶”ë¡  ì„¤ì •
â”‚   â””â”€â”€ hub_config.yaml     # Hugging Face Hub ì—…ë¡œë“œ ì„¤ì •
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/               # ë°ì´í„° ì²˜ë¦¬
â”‚   â”œâ”€â”€ model/              # ëª¨ë¸ ë¡œë”© ë° Hub ì—…ë¡œë“œ
â”‚   â”œâ”€â”€ training/           # í•™ìŠµ ëª¨ë“ˆ
â”‚   â”œâ”€â”€ evaluation/         # í‰ê°€ ë©”íŠ¸ë¦­
â”‚   â””â”€â”€ inference/          # ì¶”ë¡  íŒŒì´í”„ë¼ì¸
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py            # í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ evaluate.py         # í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ inference.py        # ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸
â”‚   â””â”€â”€ upload_to_hub.py    # Hub ì—…ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ data/
    â”œâ”€â”€ raw/                # ì›ë³¸ ë°ì´í„°
    â””â”€â”€ processed/          # ì „ì²˜ë¦¬ëœ ë°ì´í„°
```

## Usage

### 1. ë°ì´í„° ì¤€ë¹„

#### í•™ìŠµ ë°ì´í„° í˜•ì‹

í•™ìŠµ ë°ì´í„°ëŠ” JSONL í˜•ì‹ìœ¼ë¡œ ì¤€ë¹„í•©ë‹ˆë‹¤:

```json
{
  "messages": [
    {"role": "system", "content": "You are an expert Korean to English translator..."},
    {"role": "user", "content": "Translate the following...\n\n# í”„ë¡œì íŠ¸ ì†Œê°œ"},
    {"role": "assistant", "content": "# Project Introduction"}
  ]
}
```

#### ë°ì´í„° ìƒì„± ë°©ë²•

4ê°€ì§€ ë°©ë²•ìœ¼ë¡œ í•™ìŠµ ë°ì´í„°ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

##### ë°©ë²• 1: í…œí”Œë¦¿ ê¸°ë°˜ ìƒì„± (ì¶”ì²œ - API ë¶ˆí•„ìš”)

```bash
python scripts/generate_synthetic.py --method template --num-samples 1000
```

- **ì¥ì **: ë¬´ë£Œ, ë¹ ë¦„, API í‚¤ ë¶ˆí•„ìš”
- **ì¶œë ¥**: `data/synthetic/template_pairs.jsonl`
- ì‚¬ì „ ì •ì˜ëœ ë§ˆí¬ë‹¤ìš´ í…œí”Œë¦¿ìœ¼ë¡œ ë‹¤ì–‘í•œ í•œì˜ ë²ˆì—­ ìŒ ìƒì„±

##### ë°©ë²• 2: OpenAI API ê¸°ë°˜ ìƒì„±

```bash
export OPENAI_API_KEY="sk-..."

# ìˆœì°¨ ì²˜ë¦¬ (ê¸°ë³¸, ëŠë¦¼)
python scripts/generate_synthetic.py --method openai --num-samples 50 --model "gpt-4o"

# ğŸš€ ë³‘ë ¬ ì²˜ë¦¬ (ë¹ ë¦„! - ê¶Œì¥)
python scripts/generate_synthetic.py --method openai --num-samples 1000 --parallel --max-concurrent 50

# ğŸ’° Batch API (50% ë¹„ìš© ì ˆê°!)
python scripts/generate_synthetic.py --method openai --num-samples 1000 --batch --wait
```

- ê³ í’ˆì§ˆ ë²ˆì—­ ìŒ ìƒì„±
- ë‹¤ì–‘í•œ ì£¼ì œì™€ ìŠ¤íƒ€ì¼ ì§€ì›

##### ë°©ë²• 3: Anthropic API ê¸°ë°˜ ìƒì„±

```bash
export ANTHROPIC_API_KEY="sk-ant-..."

# ìˆœì°¨ ì²˜ë¦¬
python scripts/generate_synthetic.py --method anthropic --num-samples 50 --model "claude-sonnet-4-20250514"

# ğŸš€ ë³‘ë ¬ ì²˜ë¦¬
python scripts/generate_synthetic.py --method anthropic --num-samples 1000 --parallel --max-concurrent 50
```

##### ë°©ë²• 4: ìƒ˜í”Œ ë°ì´í„° ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)

```bash
python scripts/generate_synthetic.py --method sample --num-samples 20
```

- í•˜ë“œì½”ë”©ëœ ìƒ˜í”Œ ì‚¬ìš© (ë°ëª¨/í…ŒìŠ¤íŠ¸ ëª©ì )

#### ê³ ì† ë°ì´í„° ìƒì„± (ë³‘ë ¬ ì²˜ë¦¬ & Batch API)

ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¥¼ ë¹ ë¥´ê²Œ ìƒì„±í•´ì•¼ í•  ë•Œ ë‘ ê°€ì§€ ìµœì í™” ì˜µì…˜ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

##### ğŸš€ ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬ (`--parallel`)

asyncioë¥¼ ì‚¬ìš©í•˜ì—¬ ì—¬ëŸ¬ API ìš”ì²­ì„ ë™ì‹œì— ì²˜ë¦¬í•©ë‹ˆë‹¤.

```bash
# 1000ê°œ ìƒ˜í”Œì„ ë³‘ë ¬ë¡œ ìƒì„± (ìµœëŒ€ 50ê°œ ë™ì‹œ ìš”ì²­)
python scripts/generate_synthetic.py \
    --method openai \
    --num-samples 1000 \
    --parallel \
    --max-concurrent 50
```

- **ì†ë„**: ìˆœì°¨ ì²˜ë¦¬ ëŒ€ë¹„ 10-20ë°° ë¹ ë¦„
- **ë¹„ìš©**: ë™ì¼ (ì¼ë°˜ API ìš”ê¸ˆ)
- **1000ê°œ ìƒì„± ì‹œê°„**: ~15-30ë¶„

##### ğŸ’° OpenAI Batch API (`--batch`)

OpenAIì˜ Batch APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¹„ìš©ì„ ì ˆê°í•©ë‹ˆë‹¤.

```bash
# ë°°ì¹˜ ì‘ì—… ì œì¶œ (ì¦‰ì‹œ ë°˜í™˜)
python scripts/generate_synthetic.py \
    --method openai \
    --num-samples 1000 \
    --batch

# ë°°ì¹˜ ì‘ì—… ì œì¶œ ë° ì™„ë£Œ ëŒ€ê¸°
python scripts/generate_synthetic.py \
    --method openai \
    --num-samples 1000 \
    --batch \
    --wait

# ë°°ì¹˜ ìƒíƒœ í™•ì¸
python scripts/generate_synthetic.py \
    --method openai \
    --batch \
    --batch-action status \
    --batch-id batch_xxxxx

# ê²°ê³¼ ë‹¤ìš´ë¡œë“œ
python scripts/generate_synthetic.py \
    --method openai \
    --batch \
    --batch-action download \
    --batch-id batch_xxxxx
```

- **ë¹„ìš©**: 50% ì ˆê°!
- **ì†ë„**: 1-2ì‹œê°„ (24ì‹œê°„ ë‚´ ì™„ë£Œ ë³´ì¥)
- **ì œí•œ**: OpenAIë§Œ ì§€ì›

##### ì„±ëŠ¥ ë¹„êµ (1000ê°œ ìƒ˜í”Œ ê¸°ì¤€)

| ë°©ì‹ | ì‹œê°„ | ë¹„ìš© | ê¶Œì¥ ìƒí™© |
|------|------|------|-----------|
| ìˆœì°¨ ì²˜ë¦¬ | ~7ì‹œê°„ | 100% | ì†ŒëŸ‰ ë°ì´í„° |
| **ë³‘ë ¬ ì²˜ë¦¬** | ~15-30ë¶„ | 100% | **ë¹ ë¥¸ ê²°ê³¼ í•„ìš”** |
| **Batch API** | ~1-2ì‹œê°„ | **50%** | **ë¹„ìš© ì ˆê° í•„ìš”** |

#### ë°ì´í„° ì „ì²˜ë¦¬

ìƒì„±ëœ ë°ì´í„°ë¥¼ í•™ìŠµìš©ìœ¼ë¡œ ì „ì²˜ë¦¬í•˜ê³  train/valid/testë¡œ ë¶„í• í•©ë‹ˆë‹¤:

```bash
python scripts/prepare_data.py \
  --input data/raw \
  --output data/processed \
  --include-synthetic \
  --train-ratio 0.8 \
  --valid-ratio 0.1 \
  --test-ratio 0.1
```

**ì¶œë ¥ íŒŒì¼**:
- `data/processed/train.jsonl` (80%)
- `data/processed/valid.jsonl` (10%)
- `data/processed/test.jsonl` (10%)

#### ì „ì²´ íŒŒì´í”„ë¼ì¸ ì˜ˆì‹œ

```bash
# 1ë‹¨ê³„: í•©ì„± ë°ì´í„° ìƒì„±
python scripts/generate_synthetic.py --method template --num-samples 1000

# 2ë‹¨ê³„: ì „ì²˜ë¦¬ ë° ë¶„í• 
python scripts/prepare_data.py \
  --input data/raw \
  --output data/processed \
  --include-synthetic

# 3ë‹¨ê³„: í•™ìŠµ ì‹œì‘
python scripts/train.py --config configs/training_config.yaml --qlora
```

#### ë°ì´í„° ìƒì„± ë°©ë²• ë¹„êµ

| ë°©ë²• | ì†ë„ | ë¹„ìš© | í’ˆì§ˆ | API í•„ìš” |
|------|------|------|------|---------|
| í…œí”Œë¦¿ ê¸°ë°˜ | ë§¤ìš° ë¹ ë¦„ | ë¬´ë£Œ | ì¤‘ê°„ | âŒ |
| OpenAI | ëŠë¦¼ | ìœ ë£Œ | ë†’ìŒ | âœ… |
| Anthropic | ëŠë¦¼ | ìœ ë£Œ | ë†’ìŒ | âœ… |
| ìƒ˜í”Œ ë°ì´í„° | ë§¤ìš° ë¹ ë¦„ | ë¬´ë£Œ | ë‚®ìŒ | âŒ |

#### ë§ˆí¬ë‹¤ìš´ ìš”ì†Œ ì§€ì›

ìƒì„±ëœ ë°ì´í„°ëŠ” ë‹¤ì–‘í•œ ë§ˆí¬ë‹¤ìš´ ìš”ì†Œë¥¼ í¬í•¨í•˜ì—¬ ë²ˆì—­ ëª¨ë¸ì´ ë§ˆí¬ë‹¤ìš´ êµ¬ì¡°ë¥¼ ì˜ ë³´ì¡´í•˜ë„ë¡ í•™ìŠµë©ë‹ˆë‹¤.

##### ì§€ì›í•˜ëŠ” ë§ˆí¬ë‹¤ìš´ ìš”ì†Œ

| ìš”ì†Œ | ë¬¸ë²• | ì„¤ëª… |
|------|------|------|
| í—¤ë” | `# ## ###` | ë¬¸ì„œ êµ¬ì¡°í™” |
| ì½”ë“œ ë¸”ë¡ | ` ```language ``` ` | ì½”ë“œ ì˜ˆì œ |
| ì¸ë¼ì¸ ì½”ë“œ | `` `code` `` | ë³€ìˆ˜/í•¨ìˆ˜ëª… |
| í…Œì´ë¸” | `\| col \| col \|` | ë°ì´í„° ì •ë¦¬ |
| ë§í¬ | `[text](url)` | ì™¸ë¶€ ì°¸ì¡° |
| ì´ë¯¸ì§€ | `![alt](url)` | ë‹¤ì´ì–´ê·¸ë¨ |
| ë¦¬ìŠ¤íŠ¸ | `- item` / `1. item` | ëª©ë¡ |
| ì¸ìš©êµ¬ | `> quote` | ì¤‘ìš” ë‚´ìš© ê°•ì¡° |
| êµµì€ ê¸€ì”¨ | `**bold**` | ê°•ì¡° |
| ê¸°ìš¸ì„ | `*italic*` | ë¶€ê°€ ì„¤ëª… |

##### ìŠ¤íƒ€ì¼ë³„ í•„ìˆ˜ ë§ˆí¬ë‹¤ìš´ ìš”ì†Œ

ê° ë¬¸ì„œ ìŠ¤íƒ€ì¼ë§ˆë‹¤ í•„ìˆ˜ë¡œ í¬í•¨í•´ì•¼ í•˜ëŠ” ë§ˆí¬ë‹¤ìš´ ìš”ì†Œê°€ ì •ì˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤:

| ìŠ¤íƒ€ì¼ | í•„ìˆ˜ ìš”ì†Œ |
|--------|-----------|
| `readme` | í—¤ë”, ì½”ë“œ ë¸”ë¡, ë¦¬ìŠ¤íŠ¸, ë§í¬, êµµì€ ê¸€ì”¨ |
| `tutorial` | í—¤ë”, ì½”ë“œ ë¸”ë¡, ì¸ë¼ì¸ ì½”ë“œ, ë¦¬ìŠ¤íŠ¸, ë§í¬, ì¸ìš©êµ¬ |
| `api_doc` | í—¤ë”, ì½”ë“œ ë¸”ë¡, ì¸ë¼ì¸ ì½”ë“œ, í…Œì´ë¸”, ë¦¬ìŠ¤íŠ¸ |
| `blog` | í—¤ë”, ì½”ë“œ ë¸”ë¡, ë§í¬, êµµì€ ê¸€ì”¨, ê¸°ìš¸ì„, ì¸ìš©êµ¬ |
| `reference` | í—¤ë”, í…Œì´ë¸”, ì¸ë¼ì¸ ì½”ë“œ, ë¦¬ìŠ¤íŠ¸, ë§í¬ |
| `troubleshooting` | í—¤ë”, ì½”ë“œ ë¸”ë¡, ë¦¬ìŠ¤íŠ¸, ì¸ìš©êµ¬, êµµì€ ê¸€ì”¨ |

##### ìƒì„± ë°ì´í„° ê²€ì¦

OpenAI/Anthropic APIë¡œ ìƒì„±ëœ ë°ì´í„°ëŠ” ìë™ìœ¼ë¡œ ê²€ì¦ë˜ì–´ í•„ìˆ˜ ë§ˆí¬ë‹¤ìš´ ìš”ì†Œê°€ í¬í•¨ë˜ì—ˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤:

```python
# ê²€ì¦ ê²°ê³¼ ì˜ˆì‹œ
{
    "is_valid": True,
    "korean_counts": {"headers": 5, "code_blocks": 2, "tables": 1, ...},
    "english_counts": {"headers": 5, "code_blocks": 2, "tables": 1, ...},
    "required_elements": ["headers", "code_blocks", "tables"],
    "missing_korean": [],
    "missing_english": []
}
```

ë§ˆí¬ë‹¤ìš´ ë³´ì¡´ìœ¨ ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸:

```bash
python scripts/validate_markdown_preservation.py \
    --input data/processed/train.jsonl \
    --min-overall-rate 0.98
```

### 2. í•™ìŠµ

```bash
# QLoRAë¡œ í•™ìŠµ (12-16GB VRAM)
python scripts/train.py --config configs/training_config.yaml --qlora

# LoRAë¡œ í•™ìŠµ (24GB+ VRAM)
python scripts/train.py --config configs/training_config.yaml
```

### 3. í‰ê°€

```bash
python scripts/evaluate.py --adapter-path outputs/checkpoints/final/adapter
```

### 4. ì¶”ë¡ 

```bash
# ë‹¨ì¼ íŒŒì¼ ë²ˆì—­
python scripts/inference.py --input docs/README.ko.md --output docs/README.en.md

# ëŒ€í™”í˜• ëª¨ë“œ
python scripts/inference.py --interactive

# ë””ë ‰í† ë¦¬ ì¼ê´„ ë²ˆì—­
python scripts/inference.py --input-dir docs/ko --output-dir docs/en
```

### 5. Hugging Face Hub ì—…ë¡œë“œ

í•™ìŠµëœ ëª¨ë¸ì„ Hugging Face Hubì— ì—…ë¡œë“œí•˜ì—¬ ê³µìœ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### ì¸ì¦ ì„¤ì •

```bash
# ë°©ë²• 1: CLI ë¡œê·¸ì¸ (ê¶Œì¥)
huggingface-cli login

# ë°©ë²• 2: í™˜ê²½ ë³€ìˆ˜
export HF_TOKEN=hf_xxxxxxxxxxxxx
```

#### í•™ìŠµ ì™„ë£Œ í›„ ìë™ ì—…ë¡œë“œ

```bash
# í•™ìŠµ ì™„ë£Œ ì‹œ ìë™ìœ¼ë¡œ Hubì— ì—…ë¡œë“œ
python scripts/train.py --config configs/training_config.yaml --qlora \
    --push-to-hub \
    --hub-repo-id your-username/exaone-markdown-translator

# ë¹„ê³µê°œ ë¦¬í¬ì§€í† ë¦¬ë¡œ ì—…ë¡œë“œ
python scripts/train.py --config configs/training_config.yaml --qlora \
    --push-to-hub \
    --hub-repo-id your-org/internal-model \
    --hub-private
```

#### ìˆ˜ë™ ì—…ë¡œë“œ

```bash
# ì–´ëŒ‘í„°ë§Œ ì—…ë¡œë“œ (ìš©ëŸ‰ ì‘ìŒ, ì¶”ì²œ)
python scripts/upload_to_hub.py \
    --adapter-path outputs/checkpoints/final/adapter \
    --repo-id your-username/exaone-markdown-translator

# ë³‘í•©ëœ ì „ì²´ ëª¨ë¸ ì—…ë¡œë“œ (ë‹¨ë… ì‚¬ìš© ê°€ëŠ¥)
python scripts/upload_to_hub.py \
    --adapter-path outputs/checkpoints/final/adapter \
    --repo-id your-username/exaone-markdown-translator-merged \
    --merge

# ë¹„ê³µê°œ + ì»¤ìŠ¤í…€ ì„¤ì •
python scripts/upload_to_hub.py \
    --adapter-path outputs/checkpoints/final/adapter \
    --repo-id your-username/my-translator \
    --config configs/hub_config.yaml \
    --private
```

#### Hubì—ì„œ ëª¨ë¸ ì‚¬ìš©í•˜ê¸°

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# ê¸°ë³¸ ëª¨ë¸ ë¡œë“œ
base_model = AutoModelForCausalLM.from_pretrained(
    "LGAI-EXAONE/EXAONE-3.5-7.8B-Instruct",
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

# LoRA ì–´ëŒ‘í„° ë¡œë“œ
model = PeftModel.from_pretrained(
    base_model,
    "your-username/exaone-markdown-translator"
)
tokenizer = AutoTokenizer.from_pretrained(
    "your-username/exaone-markdown-translator"
)
```

## Configuration

### QLoRA ì„¤ì • (`configs/qlora_config.yaml`)

```yaml
quantization:
  load_in_4bit: true
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "bfloat16"

lora:
  r: 64
  lora_alpha: 128
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
```

### í•™ìŠµ ì„¤ì • (`configs/training_config.yaml`)

```yaml
training:
  num_train_epochs: 3
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 8
  learning_rate: 2.0e-4
```

## Evaluation Metrics

| Metric | Description |
|--------|-------------|
| BLEU | N-gram ê¸°ë°˜ ë²ˆì—­ ì •í™•ë„ |
| chrF | ë¬¸ì ìˆ˜ì¤€ F-score |
| COMET | ì‹ ê²½ë§ ê¸°ë°˜ í’ˆì§ˆ í‰ê°€ |
| Preservation Rate | ë§ˆí¬ë‹¤ìš´ ìš”ì†Œ ë³´ì¡´ìœ¨ |

## Hardware Requirements

| Method | VRAM | GPU |
|--------|------|-----|
| QLoRA (4-bit) | 12-16GB | RTX 3090, 4080 |
| LoRA (16-bit) | 20-24GB | RTX 4090, A5000 |
| Full Finetuning | 60GB+ | A100 80GB |

## License

MIT License

## References

- [EXAONE 3.5](https://github.com/LG-AI-EXAONE/EXAONE-3.5)
- [Hugging Face PEFT](https://huggingface.co/docs/peft)
- [SacreBLEU](https://github.com/mjpost/sacrebleu)
- [COMET](https://github.com/Unbabel/COMET)
