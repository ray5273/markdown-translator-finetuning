"""OpenAI Batch API ë°ì´í„° ìƒì„± ëª¨ë“ˆ

OpenAI Batch APIë¥¼ ì‚¬ìš©í•˜ì—¬ ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¥¼ ë¹„ìš© íš¨ìœ¨ì ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
- 50% ë¹„ìš© ì ˆê°
- 24ì‹œê°„ ë‚´ ì²˜ë¦¬ ì™„ë£Œ ë³´ì¥
- ìµœëŒ€ 50,000 ìš”ì²­/ë°°ì¹˜ ì§€ì›

Usage:
    from src.data.batch_generator import BatchDataGenerator

    generator = BatchDataGenerator()

    # 1. ë°°ì¹˜ íŒŒì¼ ìƒì„± ë° ì œì¶œ
    batch_id = generator.submit_batch(num_samples=1000)

    # 2. ìƒíƒœ í™•ì¸
    status = generator.check_status(batch_id)

    # 3. ê²°ê³¼ ë‹¤ìš´ë¡œë“œ (ì™„ë£Œ ì‹œ)
    results = generator.download_results(batch_id, output_path="output.jsonl")
"""

import json
import os
import random
import time
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, field
from datetime import datetime

try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False


@dataclass
class BatchJob:
    """ë°°ì¹˜ ì‘ì—… ì •ë³´"""
    batch_id: str
    input_file_id: str
    status: str
    created_at: datetime
    num_requests: int
    completed: int = 0
    failed: int = 0
    output_file_id: Optional[str] = None
    error_file_id: Optional[str] = None
    metadata: Dict = field(default_factory=dict)


@dataclass
class GeneratedPair:
    """ìƒì„±ëœ í•œì˜ ë²ˆì—­ ìŒ"""
    korean: str
    english: str
    topic: str
    style: str
    model: str
    request_id: str
    metadata: Dict = field(default_factory=dict)


class BatchDataGenerator:
    """OpenAI Batch API ë°ì´í„° ìƒì„±ê¸°

    ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¥¼ ë¹„ìš© íš¨ìœ¨ì ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
    - 50% ë¹„ìš© ì ˆê° (ê¸°ì¡´ API ëŒ€ë¹„)
    - 24ì‹œê°„ SLA
    - ë³„ë„ì˜ Rate Limit
    """

    TOPICS = [
        "Python ë¹„ë™ê¸° í”„ë¡œê·¸ë˜ë°", "JavaScript í”„ë ˆì„ì›Œí¬ ë¹„êµ", "Rust ë©”ëª¨ë¦¬ ê´€ë¦¬",
        "Go ë™ì‹œì„± íŒ¨í„´", "TypeScript íƒ€ì… ì‹œìŠ¤í…œ", "React ìƒíƒœ ê´€ë¦¬",
        "Vue.js ì»´í¬ë„ŒíŠ¸ ì„¤ê³„", "Node.js ì„±ëŠ¥ ìµœì í™”", "Docker ì»¨í…Œì´ë„ˆí™”",
        "Kubernetes í´ëŸ¬ìŠ¤í„° ê´€ë¦¬", "ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ", "ìì—°ì–´ ì²˜ë¦¬ ê¸°ì´ˆ",
        "íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜", "ëª¨ë¸ íŒŒì¸íŠœë‹ ê°€ì´ë“œ", "MLOps íŒŒì´í”„ë¼ì¸",
        "LLM í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§", "PostgreSQL ìµœì í™”", "MongoDB ìŠ¤í‚¤ë§ˆ ì„¤ê³„",
        "Redis ìºì‹± ì „ëµ", "SQL ì¿¼ë¦¬ íŠœë‹", "CI/CD íŒŒì´í”„ë¼ì¸ êµ¬ì¶•",
        "AWS ì„œë²„ë¦¬ìŠ¤ ì•„í‚¤í…ì²˜", "Terraform ì¸í”„ë¼ ì½”ë“œ", "Nginx ì„¤ì • ê°€ì´ë“œ",
        "Git ë¸Œëœì¹˜ ì „ëµ", "API ì„¤ê³„ ì›ì¹™", "ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì•„í‚¤í…ì²˜",
        "ë³´ì•ˆ ëª¨ë²” ì‚¬ë¡€", "ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§", "ë¡œê¹… ì‹œìŠ¤í…œ êµ¬ì¶•",
    ]

    STYLES = {
        "readme": "í”„ë¡œì íŠ¸ README ë¬¸ì„œ ìŠ¤íƒ€ì¼ (ì„¤ì¹˜ ë°©ë²•, ì‚¬ìš©ë²•, ì˜ˆì œ í¬í•¨)",
        "tutorial": "ë‹¨ê³„ë³„ íŠœí† ë¦¬ì–¼ ìŠ¤íƒ€ì¼ (ìƒì„¸í•œ ì„¤ëª…ê³¼ ì½”ë“œ ì˜ˆì œ)",
        "api_doc": "API ë¬¸ì„œ ìŠ¤íƒ€ì¼ (í•¨ìˆ˜ ì‹œê·¸ë‹ˆì²˜, íŒŒë¼ë¯¸í„°, ë°˜í™˜ê°’ ì„¤ëª…)",
        "blog": "ê¸°ìˆ  ë¸”ë¡œê·¸ ìŠ¤íƒ€ì¼ (ì¹œê·¼í•œ ì–´ì¡°, ê²½í—˜ ê³µìœ )",
        "reference": "ë ˆí¼ëŸ°ìŠ¤ ë¬¸ì„œ ìŠ¤íƒ€ì¼ (ê°„ê²°í•˜ê³  ì •í™•í•œ ì„¤ëª…)",
        "troubleshooting": "ë¬¸ì œ í•´ê²° ê°€ì´ë“œ ìŠ¤íƒ€ì¼ (ì¦ìƒ, ì›ì¸, í•´ê²°ì±…)",
    }

    STYLE_REQUIRED_ELEMENTS = {
        "readme": ["headers", "code_blocks", "lists", "links", "bold"],
        "tutorial": ["headers", "code_blocks", "inline_codes", "lists", "links", "blockquotes"],
        "api_doc": ["headers", "code_blocks", "inline_codes", "tables", "lists"],
        "blog": ["headers", "code_blocks", "links", "bold", "italic", "blockquotes"],
        "reference": ["headers", "tables", "inline_codes", "lists", "links"],
        "troubleshooting": ["headers", "code_blocks", "lists", "blockquotes", "bold"],
    }

    MARKDOWN_ELEMENT_DESCRIPTIONS = {
        "headers": "í—¤ë” (# ## ###)",
        "code_blocks": "ì½”ë“œ ë¸”ë¡ (```language ... ```)",
        "inline_codes": "ì¸ë¼ì¸ ì½”ë“œ (`code`)",
        "lists": "ë¦¬ìŠ¤íŠ¸ (- ë˜ëŠ” 1. 2. 3.)",
        "bold": "êµµì€ ê¸€ì”¨ (**bold**)",
        "italic": "ê¸°ìš¸ì„ (*italic*)",
        "links": "ë§í¬ ([í…ìŠ¤íŠ¸](URL))",
        "tables": "í…Œì´ë¸” (| col1 | col2 | í˜•ì‹)",
        "blockquotes": "ì¸ìš©êµ¬ (> quote)",
    }

    SYSTEM_PROMPT = """ë‹¹ì‹ ì€ ê¸°ìˆ  ë¬¸ì„œ ì „ë¬¸ ë²ˆì—­ê°€ì´ì ì‘ê°€ì…ë‹ˆë‹¤.
í•œêµ­ì–´ì™€ ì˜ì–´ ëª¨ë‘ ë„¤ì´í‹°ë¸Œ ìˆ˜ì¤€ìœ¼ë¡œ êµ¬ì‚¬í•˜ë©°,
ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ê¸°ìˆ  ë¬¸ì„œë¥¼ ì‘ì„±í•˜ëŠ” ë° ì „ë¬¸ì„±ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.

ì¤‘ìš” ê·œì¹™:
1. ë§ˆí¬ë‹¤ìš´ ë¬¸ë²•ì„ ë‹¤ì–‘í•˜ê³  ì ê·¹ì ìœ¼ë¡œ í™œìš©í•˜ì„¸ìš”
2. ì½”ë“œ ì˜ˆì œëŠ” ì‹¤ì œë¡œ ë™ì‘í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”
3. ê¸°ìˆ  ìš©ì–´ëŠ” ì •í™•í•˜ê²Œ ì‚¬ìš©í•˜ì„¸ìš”
4. í•œêµ­ì–´ ë²„ì „ê³¼ ì˜ì–´ ë²„ì „ì˜ êµ¬ì¡°ì™€ ë§ˆí¬ë‹¤ìš´ ìš”ì†ŒëŠ” ë™ì¼í•˜ê²Œ ìœ ì§€í•˜ì„¸ìš”
5. ìì—°ìŠ¤ëŸ½ê³  ì½ê¸° ì‰¬ìš´ ë¬¸ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”"""

    def __init__(
        self,
        api_key: str = None,
        model: str = "gpt-4o",
        temperature: float = 0.7
    ):
        """
        Args:
            api_key: OpenAI API í‚¤
            model: ì‚¬ìš©í•  ëª¨ë¸
            temperature: ìƒì„± ì˜¨ë„
        """
        if not OPENAI_AVAILABLE:
            raise ImportError("openai package not installed. Run: pip install openai")

        self.client = OpenAI(api_key=api_key or os.getenv("OPENAI_API_KEY"))
        self.model = model
        self.temperature = temperature
        self._batch_jobs: Dict[str, BatchJob] = {}

    def _get_required_elements_prompt(self, style: str) -> str:
        """ìŠ¤íƒ€ì¼ë³„ í•„ìˆ˜ ë§ˆí¬ë‹¤ìš´ ìš”ì†Œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        required = self.STYLE_REQUIRED_ELEMENTS.get(style, self.STYLE_REQUIRED_ELEMENTS["tutorial"])
        elements_desc = []
        for elem in required:
            if elem in self.MARKDOWN_ELEMENT_DESCRIPTIONS:
                elements_desc.append(f"   - {self.MARKDOWN_ELEMENT_DESCRIPTIONS[elem]}")
        return "\n".join(elements_desc)

    def _create_prompt(self, topic: str, style: str) -> str:
        """ìƒì„± í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        style_desc = self.STYLES.get(style, self.STYLES["tutorial"])
        required_elements = self._get_required_elements_prompt(style)

        return f"""ë‹¤ìŒ ì£¼ì œì™€ ìŠ¤íƒ€ì¼ë¡œ ë§ˆí¬ë‹¤ìš´ ê¸°ìˆ  ë¬¸ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.

ì£¼ì œ: {topic}
ìŠ¤íƒ€ì¼: {style_desc}

ìš”êµ¬ì‚¬í•­:
1. ë¨¼ì € í•œêµ­ì–´ë¡œ ì™„ì „í•œ ë¬¸ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”
2. ê·¸ ë‹¤ìŒ ë™ì¼í•œ ë‚´ìš©ì˜ ì˜ì–´ ë²ˆì—­ì„ ì‘ì„±í•˜ì„¸ìš”
3. ë‹¤ìŒ ë§ˆí¬ë‹¤ìš´ ìš”ì†Œë¥¼ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”:
{required_elements}
4. ë¬¸ì„œ ê¸¸ì´: 400-1000 ë‹¨ì–´
5. í•œêµ­ì–´ì™€ ì˜ì–´ ë²„ì „ì€ ë™ì¼í•œ ë§ˆí¬ë‹¤ìš´ êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ì„¸ìš”

ì¶œë ¥ í˜•ì‹:
---KOREAN---
[í•œêµ­ì–´ ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ]
---ENGLISH---
[ì˜ì–´ ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œ]
---END---"""

    def prepare_batch_file(
        self,
        num_samples: int,
        output_path: str = None,
        topics: List[str] = None,
        styles: List[str] = None,
        seed: int = None
    ) -> Tuple[str, List[Dict]]:
        """Batch ì…ë ¥ íŒŒì¼ ìƒì„±

        Args:
            num_samples: ìƒì„±í•  ìƒ˜í”Œ ìˆ˜
            output_path: ì…ë ¥ íŒŒì¼ ì €ì¥ ê²½ë¡œ (Noneì´ë©´ ìë™ ìƒì„±)
            topics: ì‚¬ìš©í•  ì£¼ì œ ë¦¬ìŠ¤íŠ¸
            styles: ì‚¬ìš©í•  ìŠ¤íƒ€ì¼ ë¦¬ìŠ¤íŠ¸
            seed: ëœë¤ ì‹œë“œ

        Returns:
            (íŒŒì¼ ê²½ë¡œ, ìš”ì²­ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸)
        """
        if seed is not None:
            random.seed(seed)

        topics = topics or self.TOPICS
        styles = styles or list(self.STYLES.keys())

        # ê¸°ë³¸ ê²½ë¡œ ì„¤ì • (í˜„ì¬ íŒŒì¼ ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ)
        if output_path is None:
            # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì°¾ê¸°
            current_file = Path(__file__).resolve()
            project_root = current_file.parent.parent.parent  # src/data -> src -> project_root
            output_path = project_root / "data" / "synthetic" / "batch_input.jsonl"
        else:
            output_path = Path(output_path)

        output_path.parent.mkdir(parents=True, exist_ok=True)

        requests_metadata = []

        print(f"Preparing batch input file with {num_samples} requests...")

        with open(output_path, 'w', encoding='utf-8') as f:
            iterator = range(num_samples)
            if TQDM_AVAILABLE:
                iterator = tqdm(iterator, desc="Creating requests")

            for i in iterator:
                topic = random.choice(topics)
                style = random.choice(styles)
                prompt = self._create_prompt(topic, style)

                # Batch API ìš”ì²­ í˜•ì‹
                request = {
                    "custom_id": f"request-{i:06d}",
                    "method": "POST",
                    "url": "/v1/chat/completions",
                    "body": {
                        "model": self.model,
                        "messages": [
                            {"role": "system", "content": self.SYSTEM_PROMPT},
                            {"role": "user", "content": prompt}
                        ],
                        "temperature": self.temperature
                    }
                }

                f.write(json.dumps(request) + '\n')

                # ë©”íƒ€ë°ì´í„° ì €ì¥ (ë‚˜ì¤‘ì— ê²°ê³¼ íŒŒì‹±ì— ì‚¬ìš©)
                requests_metadata.append({
                    "custom_id": f"request-{i:06d}",
                    "topic": topic,
                    "style": style
                })

        # íŒŒì¼ ì •ë³´ ì¶œë ¥
        file_size = output_path.stat().st_size
        print(f"\n=== Batch Input File Created ===")
        print(f"Path: {output_path}")
        print(f"Size: {file_size:,} bytes")
        print(f"Requests: {num_samples}")

        # ì²« ë²ˆì§¸ ìš”ì²­ ìƒ˜í”Œ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
        with open(output_path, 'r', encoding='utf-8') as f:
            first_line = f.readline()
            sample = json.loads(first_line)
            print(f"\nSample request (first line):")
            print(f"  custom_id: {sample.get('custom_id')}")
            print(f"  method: {sample.get('method')}")
            print(f"  url: {sample.get('url')}")
            print(f"  body.model: {sample.get('body', {}).get('model')}")
            print(f"  body.messages: {len(sample.get('body', {}).get('messages', []))} messages")

        return str(output_path), requests_metadata

    def validate_batch_file(self, file_path: str) -> Tuple[bool, List[str]]:
        """ë°°ì¹˜ ì…ë ¥ íŒŒì¼ ê²€ì¦

        Args:
            file_path: ê²€ì¦í•  JSONL íŒŒì¼ ê²½ë¡œ

        Returns:
            (is_valid, errors) íŠœí”Œ
        """
        errors = []
        line_count = 0

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for i, line in enumerate(f, 1):
                    line_count = i
                    if not line.strip():
                        continue

                    try:
                        request = json.loads(line)

                        # í•„ìˆ˜ í•„ë“œ í™•ì¸
                        if "custom_id" not in request:
                            errors.append(f"Line {i}: Missing 'custom_id'")
                        if "method" not in request:
                            errors.append(f"Line {i}: Missing 'method'")
                        if "url" not in request:
                            errors.append(f"Line {i}: Missing 'url'")
                        if "body" not in request:
                            errors.append(f"Line {i}: Missing 'body'")

                        # body í•„ë“œ í™•ì¸
                        if "body" in request:
                            body = request["body"]
                            if "model" not in body:
                                errors.append(f"Line {i}: Missing 'model' in body")
                            if "messages" not in body:
                                errors.append(f"Line {i}: Missing 'messages' in body")

                    except json.JSONDecodeError as e:
                        errors.append(f"Line {i}: Invalid JSON - {e}")

        except FileNotFoundError:
            errors.append(f"File not found: {file_path}")

        is_valid = len(errors) == 0

        print(f"\n=== Batch File Validation ===")
        print(f"File: {file_path}")
        print(f"Total lines: {line_count}")
        print(f"Valid: {'âœ… Yes' if is_valid else 'âŒ No'}")

        if errors:
            print(f"\nErrors ({len(errors)}):")
            for error in errors[:10]:  # ìµœëŒ€ 10ê°œë§Œ í‘œì‹œ
                print(f"  - {error}")
            if len(errors) > 10:
                print(f"  ... and {len(errors) - 10} more errors")

        return is_valid, errors

    def submit_batch(
        self,
        num_samples: int = None,
        input_file_path: str = None,
        metadata_path: str = None,
        topics: List[str] = None,
        styles: List[str] = None
    ) -> str:
        """Batch ì‘ì—… ì œì¶œ

        Args:
            num_samples: ìƒì„±í•  ìƒ˜í”Œ ìˆ˜ (input_file_pathê°€ ì—†ì„ ë•Œ)
            input_file_path: ì´ë¯¸ ìƒì„±ëœ ì…ë ¥ íŒŒì¼ ê²½ë¡œ
            metadata_path: ë©”íƒ€ë°ì´í„° ì €ì¥ ê²½ë¡œ
            topics: ì‚¬ìš©í•  ì£¼ì œ ë¦¬ìŠ¤íŠ¸
            styles: ì‚¬ìš©í•  ìŠ¤íƒ€ì¼ ë¦¬ìŠ¤íŠ¸

        Returns:
            batch_id
        """
        # ì…ë ¥ íŒŒì¼ ì¤€ë¹„
        if input_file_path is None:
            if num_samples is None:
                raise ValueError("Either num_samples or input_file_path must be provided")

            input_file_path, requests_metadata = self.prepare_batch_file(
                num_samples=num_samples,
                topics=topics,
                styles=styles
            )

            # ë©”íƒ€ë°ì´í„° ì €ì¥
            if metadata_path is None:
                metadata_path = input_file_path.replace('.jsonl', '_metadata.json')

            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(requests_metadata, f, ensure_ascii=False, indent=2)
            print(f"Metadata saved: {metadata_path}")
        else:
            requests_metadata = []

        # ì…ë ¥ íŒŒì¼ ê²€ì¦
        print("\nValidating batch input file...")
        is_valid, validation_errors = self.validate_batch_file(input_file_path)
        if not is_valid:
            raise ValueError(f"Batch input file validation failed. Fix errors and try again.")

        # íŒŒì¼ ì—…ë¡œë“œ
        print("\nUploading batch file to OpenAI...")
        with open(input_file_path, 'rb') as f:
            batch_file = self.client.files.create(
                file=f,
                purpose="batch"
            )
        print(f"File uploaded: {batch_file.id}")

        # Batch ì‘ì—… ìƒì„±
        print("Creating batch job...")
        batch = self.client.batches.create(
            input_file_id=batch_file.id,
            endpoint="/v1/chat/completions",
            completion_window="24h",
            metadata={
                "description": f"Synthetic data generation - {num_samples or 'custom'} samples"
            }
        )

        # ì‘ì—… ì •ë³´ ì €ì¥
        job = BatchJob(
            batch_id=batch.id,
            input_file_id=batch_file.id,
            status=batch.status,
            created_at=datetime.now(),
            num_requests=num_samples or len(requests_metadata),
            metadata={"input_file_path": input_file_path, "metadata_path": metadata_path}
        )
        self._batch_jobs[batch.id] = job

        print(f"\n=== Batch Job Submitted ===")
        print(f"Batch ID: {batch.id}")
        print(f"Status: {batch.status}")
        print(f"Input file: {batch_file.id}")
        print(f"\nTo check status, run:")
        print(f"  generator.check_status('{batch.id}')")

        return batch.id

    def check_status(self, batch_id: str, verbose: bool = True) -> Dict:
        """Batch ìƒíƒœ í™•ì¸

        Args:
            batch_id: ë°°ì¹˜ ì‘ì—… ID
            verbose: ìƒì„¸ ì •ë³´ ì¶œë ¥ ì—¬ë¶€

        Returns:
            ìƒíƒœ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        batch = self.client.batches.retrieve(batch_id)

        status = {
            "batch_id": batch.id,
            "status": batch.status,
            "created_at": batch.created_at,
            "completed": batch.request_counts.completed if batch.request_counts else 0,
            "failed": batch.request_counts.failed if batch.request_counts else 0,
            "total": batch.request_counts.total if batch.request_counts else 0,
            "output_file_id": batch.output_file_id,
            "error_file_id": batch.error_file_id,
            "input_file_id": batch.input_file_id,
            "errors": None,
        }

        # ì—ëŸ¬ ì •ë³´ ì¶”ì¶œ
        if hasattr(batch, 'errors') and batch.errors:
            status["errors"] = batch.errors

        # ë¡œì»¬ ìºì‹œ ì—…ë°ì´íŠ¸
        if batch_id in self._batch_jobs:
            self._batch_jobs[batch_id].status = batch.status
            self._batch_jobs[batch_id].completed = status["completed"]
            self._batch_jobs[batch_id].failed = status["failed"]
            self._batch_jobs[batch_id].output_file_id = batch.output_file_id
            self._batch_jobs[batch_id].error_file_id = batch.error_file_id

        # ì§„í–‰ë¥  ê³„ì‚°
        if status["total"] > 0:
            progress = (status["completed"] + status["failed"]) / status["total"] * 100
            status["progress"] = f"{progress:.1f}%"

        if verbose:
            print(f"\n=== Batch Status ===")
            print(f"Batch ID: {batch_id}")
            print(f"Status: {status['status']}")
            print(f"Input File: {status['input_file_id']}")
            print(f"Progress: {status.get('progress', 'N/A')}")
            print(f"Completed: {status['completed']}/{status['total']}")
            print(f"Failed: {status['failed']}")

            # ì—ëŸ¬ ì •ë³´ ì¶œë ¥
            if status["errors"]:
                print(f"\nâš ï¸  Errors:")
                if hasattr(status["errors"], 'data'):
                    for error in status["errors"].data:
                        print(f"  - {error.code}: {error.message}")
                else:
                    print(f"  {status['errors']}")

            # ìƒíƒœë³„ ì•ˆë‚´ ë©”ì‹œì§€
            if status["status"] == "validating":
                print(f"\nğŸ“‹ Batch is being validated. This may take a few minutes...")
            elif status["status"] == "in_progress":
                print(f"\nğŸš€ Batch is processing...")
            elif status["status"] == "completed":
                print(f"\nâœ… Batch completed! Use download_results() to get results.")
            elif status["status"] == "failed":
                print(f"\nâŒ Batch failed. Check errors above.")
            elif status["status"] == "expired":
                print(f"\nâ° Batch expired (24h limit reached).")
            elif status["status"] == "cancelled":
                print(f"\nğŸš« Batch was cancelled.")

        return status

    def wait_for_completion(
        self,
        batch_id: str,
        poll_interval: int = 30,
        timeout: int = 86400  # 24 hours
    ) -> Dict:
        """Batch ì™„ë£Œ ëŒ€ê¸°

        Args:
            batch_id: ë°°ì¹˜ ì‘ì—… ID
            poll_interval: ìƒíƒœ í™•ì¸ ê°„ê²© (ì´ˆ)
            timeout: ìµœëŒ€ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)

        Returns:
            ìµœì¢… ìƒíƒœ ì •ë³´
        """
        print(f"\nWaiting for batch {batch_id} to complete...")
        print(f"Polling interval: {poll_interval}s")
        print(f"Timeout: {timeout}s")
        print()

        start_time = time.time()
        last_completed = 0
        last_status = None

        while True:
            elapsed = time.time() - start_time
            if elapsed > timeout:
                raise TimeoutError(f"Batch job timed out after {timeout}s")

            # ìƒíƒœ ë³€ê²½ ì‹œì—ë§Œ ìƒì„¸ ì¶œë ¥
            status = self.check_status(batch_id, verbose=False)

            # ìƒíƒœ ë³€ê²½ ì‹œ ì¶œë ¥
            if status["status"] != last_status:
                elapsed_min = elapsed / 60
                print(f"[{elapsed_min:.1f}m] Status: {status['status']} | "
                      f"Progress: {status['completed']}/{status['total']}")
                last_status = status["status"]

                # ì—ëŸ¬ê°€ ìˆìœ¼ë©´ ì¶œë ¥
                if status.get("errors"):
                    print(f"  âš ï¸  Errors detected:")
                    if hasattr(status["errors"], 'data'):
                        for error in status["errors"].data:
                            print(f"    - {error.code}: {error.message}")

            # ì§„í–‰ ìƒí™© ì¶œë ¥ (ì²˜ë¦¬ ì¤‘ì¸ ê²½ìš°)
            elif status["completed"] > last_completed:
                rate = (status["completed"] - last_completed) / poll_interval
                elapsed_min = elapsed / 60
                print(f"[{elapsed_min:.1f}m] Progress: {status['completed']}/{status['total']} "
                      f"(~{rate:.1f} req/s)")
                last_completed = status["completed"]

            if status["status"] == "completed":
                print(f"\nâœ… Batch completed successfully!")
                print(f"   Total time: {elapsed/60:.1f} minutes")
                return status
            elif status["status"] == "failed":
                # ì‹¤íŒ¨ ì‹œ ìƒì„¸ ì •ë³´ ì¶œë ¥
                self.check_status(batch_id, verbose=True)
                raise RuntimeError(f"Batch job failed. Check errors above.")
            elif status["status"] == "cancelled":
                raise RuntimeError(f"Batch job was cancelled")
            elif status["status"] == "expired":
                raise RuntimeError(f"Batch job expired (24h limit reached)")

            # ëŒ€ê¸°
            time.sleep(poll_interval)

    def _parse_response(self, response_text: str, topic: str, style: str, request_id: str) -> Optional[GeneratedPair]:
        """ì‘ë‹µ íŒŒì‹±"""
        if "---KOREAN---" in response_text and "---ENGLISH---" in response_text:
            korean_start = response_text.find("---KOREAN---") + len("---KOREAN---")
            korean_end = response_text.find("---ENGLISH---")
            english_start = korean_end + len("---ENGLISH---")
            english_end = response_text.find("---END---") if "---END---" in response_text else len(response_text)

            korean = response_text[korean_start:korean_end].strip()
            english = response_text[english_start:english_end].strip()

            if korean and english:
                return GeneratedPair(
                    korean=korean,
                    english=english,
                    topic=topic,
                    style=style,
                    model=self.model,
                    request_id=request_id
                )
        return None

    def download_results(
        self,
        batch_id: str,
        output_path: str = "data/synthetic/batch_output.jsonl",
        metadata_path: str = None
    ) -> List[GeneratedPair]:
        """ê²°ê³¼ ë‹¤ìš´ë¡œë“œ ë° íŒŒì‹±

        Args:
            batch_id: ë°°ì¹˜ ì‘ì—… ID
            output_path: ê²°ê³¼ ì €ì¥ ê²½ë¡œ
            metadata_path: ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ (topic, style ì •ë³´)

        Returns:
            ìƒì„±ëœ ìŒ ë¦¬ìŠ¤íŠ¸
        """
        # ìƒíƒœ í™•ì¸
        batch = self.client.batches.retrieve(batch_id)

        if batch.status != "completed":
            raise ValueError(f"Batch not completed. Current status: {batch.status}")

        if not batch.output_file_id:
            raise ValueError("No output file available")

        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        metadata_map = {}
        if metadata_path:
            try:
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata_list = json.load(f)
                    for item in metadata_list:
                        metadata_map[item["custom_id"]] = item
            except FileNotFoundError:
                print(f"Warning: Metadata file not found: {metadata_path}")

        # ë¡œì»¬ ìºì‹œì—ì„œ ë©”íƒ€ë°ì´í„° ê²½ë¡œ í™•ì¸
        if not metadata_map and batch_id in self._batch_jobs:
            cached_metadata_path = self._batch_jobs[batch_id].metadata.get("metadata_path")
            if cached_metadata_path:
                try:
                    with open(cached_metadata_path, 'r', encoding='utf-8') as f:
                        metadata_list = json.load(f)
                        for item in metadata_list:
                            metadata_map[item["custom_id"]] = item
                except FileNotFoundError:
                    pass

        # ê²°ê³¼ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
        print(f"Downloading results from {batch.output_file_id}...")
        result_content = self.client.files.content(batch.output_file_id)

        output_path = Path(output_path)
        output_path.parent.mkdir(parents=True, exist_ok=True)

        results = []
        successful = 0
        failed = 0

        with open(output_path, 'w', encoding='utf-8') as f:
            for line in result_content.text.strip().split('\n'):
                if not line:
                    continue

                try:
                    result = json.loads(line)
                    custom_id = result.get("custom_id", "")

                    # ë©”íƒ€ë°ì´í„°ì—ì„œ topic, style ê°€ì ¸ì˜¤ê¸°
                    meta = metadata_map.get(custom_id, {})
                    topic = meta.get("topic", "unknown")
                    style = meta.get("style", "unknown")

                    # ì‘ë‹µ ì¶”ì¶œ
                    if result.get("response", {}).get("status_code") == 200:
                        body = result["response"]["body"]
                        content = body["choices"][0]["message"]["content"]

                        pair = self._parse_response(content, topic, style, custom_id)

                        if pair:
                            results.append(pair)
                            successful += 1

                            # JSONL í˜•ì‹ìœ¼ë¡œ ì €ì¥
                            data = {
                                "korean": pair.korean,
                                "english": pair.english,
                                "metadata": {
                                    "topic": pair.topic,
                                    "style": pair.style,
                                    "model": pair.model,
                                    "source": "synthetic_batch",
                                    "request_id": pair.request_id
                                }
                            }
                            f.write(json.dumps(data, ensure_ascii=False) + '\n')
                        else:
                            failed += 1
                    else:
                        failed += 1
                        error = result.get("error", {})
                        print(f"Request {custom_id} failed: {error}")

                except json.JSONDecodeError as e:
                    print(f"Failed to parse result line: {e}")
                    failed += 1

        # ì—ëŸ¬ íŒŒì¼ ì²˜ë¦¬
        if batch.error_file_id:
            print(f"\nDownloading error file...")
            error_content = self.client.files.content(batch.error_file_id)
            error_path = str(output_path).replace('.jsonl', '_errors.jsonl')
            with open(error_path, 'w', encoding='utf-8') as f:
                f.write(error_content.text)
            print(f"Errors saved to: {error_path}")

        print(f"\n=== Download Complete ===")
        print(f"Successful: {successful}")
        print(f"Failed: {failed}")
        print(f"Output: {output_path}")

        return results

    def list_batches(self, limit: int = 10) -> List[Dict]:
        """ìµœê·¼ ë°°ì¹˜ ì‘ì—… ëª©ë¡ ì¡°íšŒ

        Args:
            limit: ì¡°íšŒí•  ìµœëŒ€ ê°œìˆ˜

        Returns:
            ë°°ì¹˜ ì‘ì—… ëª©ë¡
        """
        batches = self.client.batches.list(limit=limit)

        results = []
        print(f"\n=== Recent Batch Jobs ===")
        for batch in batches.data:
            info = {
                "batch_id": batch.id,
                "status": batch.status,
                "created_at": batch.created_at,
                "completed": batch.request_counts.completed if batch.request_counts else 0,
                "total": batch.request_counts.total if batch.request_counts else 0,
            }
            results.append(info)
            print(f"  {batch.id}: {batch.status} ({info['completed']}/{info['total']})")

        return results

    def cancel_batch(self, batch_id: str) -> Dict:
        """ë°°ì¹˜ ì‘ì—… ì·¨ì†Œ

        Args:
            batch_id: ë°°ì¹˜ ì‘ì—… ID

        Returns:
            ì·¨ì†Œëœ ë°°ì¹˜ ì •ë³´
        """
        batch = self.client.batches.cancel(batch_id)
        print(f"Batch {batch_id} cancellation requested. Status: {batch.status}")
        return {"batch_id": batch.id, "status": batch.status}


def run_batch_generation(
    num_samples: int,
    output_path: str = None,
    api_key: str = None,
    model: str = "gpt-4o",
    wait: bool = True,
    poll_interval: int = 30
) -> Tuple[str, Optional[List[GeneratedPair]]]:
    """ë°°ì¹˜ ìƒì„± ì‹¤í–‰ (CLIìš© ë˜í¼)

    Args:
        num_samples: ìƒì„±í•  ìƒ˜í”Œ ìˆ˜
        output_path: ê²°ê³¼ ì €ì¥ ê²½ë¡œ
        api_key: OpenAI API í‚¤
        model: ì‚¬ìš©í•  ëª¨ë¸
        wait: ì™„ë£Œê¹Œì§€ ëŒ€ê¸°í• ì§€ ì—¬ë¶€
        poll_interval: ìƒíƒœ í™•ì¸ ê°„ê²©

    Returns:
        (batch_id, results) - wait=Falseë©´ resultsëŠ” None
    """
    output_path = output_path or f"data/synthetic/batch_output.jsonl"

    generator = BatchDataGenerator(api_key=api_key, model=model)

    # ë°°ì¹˜ ì œì¶œ
    batch_id = generator.submit_batch(num_samples=num_samples)

    if not wait:
        print(f"\nBatch submitted. To check status later:")
        print(f"  python -c \"from src.data.batch_generator import BatchDataGenerator; g = BatchDataGenerator(); g.check_status('{batch_id}')\"")
        return batch_id, None

    # ì™„ë£Œ ëŒ€ê¸°
    generator.wait_for_completion(batch_id, poll_interval=poll_interval)

    # ê²°ê³¼ ë‹¤ìš´ë¡œë“œ
    results = generator.download_results(batch_id, output_path=output_path)

    return batch_id, results


if __name__ == "__main__":
    import sys

    if len(sys.argv) < 2:
        print("Usage:")
        print("  python batch_generator.py submit <num_samples>")
        print("  python batch_generator.py status <batch_id>")
        print("  python batch_generator.py download <batch_id> [output_path]")
        print("  python batch_generator.py list")
        sys.exit(1)

    command = sys.argv[1]
    generator = BatchDataGenerator()

    if command == "submit":
        num_samples = int(sys.argv[2]) if len(sys.argv) > 2 else 100
        generator.submit_batch(num_samples=num_samples)

    elif command == "status":
        batch_id = sys.argv[2]
        generator.check_status(batch_id)

    elif command == "download":
        batch_id = sys.argv[2]
        output_path = sys.argv[3] if len(sys.argv) > 3 else None
        generator.download_results(batch_id, output_path=output_path)

    elif command == "list":
        generator.list_batches()

    elif command == "cancel":
        batch_id = sys.argv[2]
        generator.cancel_batch(batch_id)

    else:
        print(f"Unknown command: {command}")
        sys.exit(1)
